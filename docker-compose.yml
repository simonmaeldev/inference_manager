services:
  inference:
    build: .
    ports:
      - "7860:7860"  # Stable Diffusion Web UI
      - "8000:8000"  # llama-cpp-python API
    volumes:
      - "C:/Users/ApprenTyr/.lmstudio/models/:/app/models/"
      - "C:/Users/ApprenTyr/Documents/StabilityMatrix-win-x64/Data/Models/StableDiffusion:/app/stable-diffusion-webui/models/Stable-diffusion"
      - "C:/Users/ApprenTyr/Documents/StabilityMatrix-win-x64/Data/Models/Lora:/app/stable-diffusion-webui/models/Lora"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
